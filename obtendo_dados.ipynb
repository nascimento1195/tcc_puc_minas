{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d39b72a",
   "metadata": {},
   "source": [
    "# Instalação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e8c0f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install tweepy\n",
    "!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a50666",
   "metadata": {},
   "source": [
    "# Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0a9b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067272b5",
   "metadata": {},
   "source": [
    "# Extraindo dados do Reclame Aqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfb9b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_informacoes(base_url, empresa, driver, n_paginas):\n",
    "    url = base_url + empresa + \"/lista-reclamacoes/?pagina=\"\n",
    "    reclamacoes, titulos, links = [], [], []\n",
    "\n",
    "    for i in range(1, n_paginas + 1):\n",
    "        pag = url + str(i)\n",
    "        driver.get(pag)\n",
    "        sleep(3)\n",
    "        html = bs(driver.page_source, \"html.parser\")\n",
    "\n",
    "        reclamacoes_html = html.find_all(\"p\", {\"class\": 'text-detail'})\n",
    "        reclamacoes_na_pagina = [\n",
    "            reclamacao.text.split(\"|\") for reclamacao in reclamacoes_html\n",
    "        ]\n",
    "        reclamacoes.extend(reclamacoes_na_pagina)\n",
    "\n",
    "        titulos_e_links = html.find_all(\n",
    "            \"a\", {\"class\": \"link-complain-id-complains\"}\n",
    "        )\n",
    "\n",
    "        titulos.extend([titulo.text.strip() for titulo in titulos_e_links])\n",
    "        links.extend([link.get(\"href\") for link in titulos_e_links])\n",
    "\n",
    "    return reclamacoes, titulos, links\n",
    "\n",
    "\n",
    "def extrair_descricoes(base_url, links, driver):\n",
    "    urls = [base_url + link[1:] for link in links]\n",
    "    descricoes = []\n",
    "    notas = []\n",
    "    for url in urls:\n",
    "        driver.get(url)\n",
    "        sleep(3)\n",
    "        html = bs(driver.page_source, \"html.parser\")\n",
    "        descricao_aux = html.find(\"div\", {\"class\": \"complain-body\"})\n",
    "        if descricao_aux is None:\n",
    "            descricoes.append('Comentario Nulo')\n",
    "        else:\n",
    "            descricao = descricao_aux.text.strip()\n",
    "            descricoes.append(descricao)\n",
    "\n",
    "    return descricoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c741c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = webdriver.Chrome('chromedriver')\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c4d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.reclameaqui.com.br/empresa/'\n",
    "empresa = 'mercado-pago'\n",
    "\n",
    "r, t, l = extrair_informacoes(base_url, empresa, driver, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e361f21",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "d = extrair_descricoes(base_url, l, driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f48d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246ebb33",
   "metadata": {},
   "source": [
    "# Extraindo dados do Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed5e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = '4Q60fmY53FcDWLPrmFLzMflqg'\n",
    "api_secret_key = '8tJ8EKQyLE7hR2sUJ8RsFVpjvRoaHqWICwQfByFtwGHXTxFoic'\n",
    "\n",
    "access_token = '1230518026015039488-Lof1DYgj97LOSViIzuh4hHgSrLudyf'\n",
    "access_token_secret = 'BO8e5kIIrw5O4pX8GV2zkbZNwGZtLOmcZA9V4qMcc3X4K'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cd1b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(api_key, api_secret_key)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a2b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query = '@MercadoPagoBr'\n",
    "count = 200\n",
    "\n",
    "try:\n",
    "    # Creation of query method using parameters\n",
    "    tweets = tweepy.Cursor(api.search, q=text_query).items(count)\n",
    " \n",
    "    # Pulling information from tweets iterable object\n",
    "    tweets_list = [[tweet.created_at, tweet.id, tweet.text] for tweet in tweets]\n",
    " \n",
    "    # Creation of dataframe from tweets list\n",
    "    # Add or remove columns as you remove tweet information\n",
    "    tweets_df = pd.DataFrame(tweets_list)\n",
    "\n",
    "except BaseException as e:\n",
    "    print('failed on_status,', str(e))\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9156cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.columns = ['data', 'id', 'reclamacao']\n",
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a15a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71066f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['titulo'] = 'Twitter'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7939d5",
   "metadata": {},
   "source": [
    "# Salvando DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eb0255",
   "metadata": {},
   "outputs": [],
   "source": [
    "reclamacoes_df = pd.DataFrame({'titulo': t, 'reclamacao': d})\n",
    "reclamacoes_df.to_csv('base_reclame_aqui.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb30137",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[['titulo', 'reclamacao']].to_csv('base_twitter.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('puc': conda)",
   "language": "python",
   "name": "python37164bitpuccondaa633a1be76c640dfacaa0ddb898b0876"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
